import streamlit as st
import pandas as pd
import requests
import io
import re
from collections import Counter
import pymorphy3
import base64
from io import BytesIO
from docx import Document
import os
from dotenv import load_dotenv
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from urllib.parse import urlparse, urljoin
from typing import List, Dict, Tuple, Optional, Any, Set
from bs4 import BeautifulSoup
from pymystem3 import Mystem
import Stemmer
from rapidfuzz import fuzz
import html
import traceback

# =================== –ù–ê–°–¢–†–û–ô–ö–ò ===================
load_dotenv()
st.set_page_config(page_title="SEO-–∫–æ–º–±–∞–π–Ω", layout="wide", page_icon="üßô‚Äç‚ôÇÔ∏è")

API_TOKEN = "13aef476b8d8f6252959163f5d49812b"
API_URL = f"https://api.serpstat.com/v4?token={API_TOKEN}" if API_TOKEN else None

AUTO_EXTEND = {
    "always": "always –ø—Ä–æ–∫–ª–∞–¥–∫–∏",
    "–æ–ª–≤–µ–π—Å": "–æ–ª–≤–µ–π—Å –ø—Ä–æ–∫–ª–∞–¥–∫–∏",
    "libresse": "libresse –ø—Ä–æ–∫–ª–∞–¥–∫–∏",
    "nurofen": "nurofen —Ç–∞–±–ª–µ—Ç–∫–∏",
    "–Ω–æ-—à–ø–∞": "–Ω–æ-—à–ø–∞ —Ç–∞–±–ª–µ—Ç–∫–∏",
    "no-shpa": "no-shpa —Ç–∞–±–ª–µ—Ç–∫–∏",
}

RELEVANT_WORDS = [
    "–ø—Ä–æ–∫–ª–∞–¥", "–µ–∂–µ–¥–Ω–µ–≤", "–Ω–æ—á–Ω", "–≥–∏–≥–∏–µ–Ω", "always", "–æ–ª–≤–µ–π—Å",
    "—Ç–∞–±–ª–µ—Ç", "nurofen", "–Ω–æ-—à–ø–∞", "libresse", "no-shpa"
]

# --- –ö–û–ù–°–¢–ê–ù–¢–´ –î–õ–Ø –ù–û–í–û–ô –í–ö–õ–ê–î–ö–ò ---
COL_URL_RU_EXCEL = '–¶–µ–ª–µ–≤–æ–π URL (RU)'
COL_TITLE_RU_EXCEL = 'Title RU'
COL_DESC_RU_EXCEL = 'Description RU'
COL_TITLE_UA_EXCEL = 'Title UA'
COL_DESC_UA_EXCEL = 'Description UA'
COL_EXACT_PHRASES_RU_EXCEL = '–§—Ä–∞–∑—ã –≤ —Ç–æ—á–Ω–æ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–∏ RU'
COL_EXACT_PHRASES_UA_EXCEL = '–§—Ä–∞–∑—ã –≤ —Ç–æ—á–Ω–æ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–∏ UA'
COL_LSI_RU_EXCEL = 'LSI'
COL_LSI_UA_EXCEL = 'LSI UA'
PREFIX_EXACT_PHRASE = "üîç "
PREFIX_LSI_PHRASE = "üî§ "
BASE_URL_SITE = 'https://apteka911.ua'
DEFAULT_ENABLE_LSI_TRUNCATION = True
DEFAULT_LSI_TRUNC_MAX_REMOVE = 3
DEFAULT_LSI_TRUNC_MIN_ORIG_LEN = 7
DEFAULT_LSI_TRUNC_MIN_FINAL_LEN = 4
DEFAULT_STEM_FUZZY_RATIO_THRESHOLD = 90

# --- –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ---
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
RUSSIAN_STEMMER = Stemmer.Stemmer('russian')
RAPIDFUZZ_AVAILABLE = True

@st.cache_resource
def get_mystem_instance():
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Mystem...")
    return Mystem()
morph = get_mystem_instance()

session = requests.Session()
retry_strategy = Retry(total=3, backoff_factor=0.5, status_forcelist=[500, 502, 503, 504])
adapter = HTTPAdapter(max_retries=retry_strategy)
session.mount('http://', adapter)
session.mount('https://', adapter)

# ========== –§–£–ù–ö–¶–ò–ò –ò–ó –ü–†–ï–î–´–î–£–©–ï–ì–û –ö–û–î–ê ==========
def build_query(name):
    n = name.lower().strip()
    for key in AUTO_EXTEND:
        if key in n:
            return AUTO_EXTEND[key]
    return name

def filter_phrases(phrases, relevant_words=RELEVANT_WORDS, top_n=10):
    filtered = []
    for p in phrases:
        lp = p.lower()
        word_count = len(lp.split())
        if word_count >= 1 and any(word in lp for word in relevant_words):  # –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä
            filtered.append(p)
        if len(filtered) >= top_n:
            break
    return filtered[:top_n] if filtered else phrases[:top_n]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ö–æ—Ç—è –±—ã —Ç–æ–ø-N, –µ—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä –ø—É—Å—Ç

def get_serpstat_phrases_top_filtered(keyword, top_n=10):
    if not API_TOKEN:
        st.error("API-—Ç–æ–∫–µ–Ω Serpstat –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å —Ñ–∞–π–ª .env.")
        return []
    keyword_query = build_query(keyword)
    payload = {
        "id": "1",
        "method": "SerpstatKeywordProcedure.getKeywords",
        "params": {
            "keyword": keyword_query,
            "se": "g_ua",
            "type": "phrase_all",
            "page": 1,
            "size": 500
        }
    }
    try:
        st.write(f"–ó–∞–ø—Ä–æ—Å –∫ API –¥–ª—è –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞: {keyword_query}")
        resp = requests.post(API_URL, json=payload, timeout=30)
        resp.raise_for_status()
        result = resp.json()
        st.write(f"–û—Ç–≤–µ—Ç –æ—Ç API: {result}")
        if "result" in result and "data" in result["result"]:
            data = result["result"]["data"]
            data = [d for d in data if "keyword" in d and "region_queries_count" in d]
            if not data:
                st.write("API –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö.")
                return []
            data.sort(key=lambda d: int(d["region_queries_count"]), reverse=True)
            all_phrases = [d['keyword'] for d in data]
            filtered_phrases = filter_phrases(all_phrases, RELEVANT_WORDS, top_n)
            st.write(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã: {filtered_phrases}")
            return filtered_phrases
        else:
            st.write("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ API.")
            return []
    except requests.exceptions.RequestException as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ API Serpstat: {str(e)}")
        return []
    except Exception as e:
        st.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {str(e)}")
        return []

def analyze_texts(text1: str, text2: str) -> str:
    try:
        morph = pymorphy3.MorphAnalyzer()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        return "–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑."

    text1_cleaned = re.sub(r'\s+', ' ', text1)
    text2_cleaned = re.sub(r'\s+', ' ', text2)
    def get_lemmas(text: str) -> list:
        words = re.findall(r'\b[–∞-—è–ê-–Ø—ë–Å-]+\b', text.lower())
        lemmas = [morph.parse(word)[0].normal_form for word in words]
        return lemmas

    lemmas1_set = set(get_lemmas(text1_cleaned))
    all_lemmas_from_text2 = get_lemmas(text2_cleaned)
    unique_lemmas = [lemma for lemma in all_lemmas_from_text2 if lemma not in lemmas1_set]
    if not unique_lemmas:
        return "–í–æ –≤—Ç–æ—Ä–æ–º —Ç–µ–∫—Å—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤ –ø–µ—Ä–≤–æ–º."
    frequency_counter = Counter(unique_lemmas)
    sorted_lemmas = sorted(frequency_counter.items(), key=lambda x: (-x[1], x[0]))
    result_markdown = [f"- **{word}** ‚Äî {count} —Ä–∞–∑(–∞)" for word, count in sorted_lemmas[:300]]
    return "\n".join(result_markdown)

# ========== –ù–û–í–ê–Ø –í–ö–õ–ê–î–ö–ê: SEO Meta Checker ==========
def normalize_for_search(text: Optional[Any]) -> str:
    if not text: return ""
    text_str = str(text).lower()
    text_str = re.sub(r'(—Ü–µ–Ω–∞ –æ—Ç|—Ü—ñ–Ω–∞ –≤—ñ–¥|price from)\s*[^-\|\n\r<]+?(\s*[-\|]|$)', ' ', text_str, flags=re.IGNORECASE)
    text_str = re.sub(r'%[a-z_]+price[a-z_]*%', ' ', text_str)
    text_str = re.sub(r'%[a-z_]+%', ' ', text_str)
    text_str = re.sub(r'\d+(\.\d+)?\s*(–≥—Ä–Ω|uah|usd|eur)?', ' ', text_str)
    text_str = re.sub(r'[/\\|\-‚Äì‚Äî]', ' ', text_str)
    text_str = re.sub(r'[\s\u00a0\u200b]+', ' ', text_str)
    text_str = re.sub(r'[^\w\s]', '', text_str, flags=re.UNICODE)
    text_str = re.sub(r'\s+', ' ', text_str)
    return text_str.strip()

def split_phrases(phrases_text: Optional[Any]) -> List[str]:
    if not phrases_text or (isinstance(phrases_text, float) and pd.isna(phrases_text)) or not str(phrases_text).strip():
        return []
    phrases = re.split(r'[,\n]+', str(phrases_text))
    return [p.strip() for p in phrases if p.strip()]

@st.cache_data
def cached_lemmatize_word_flexibly(word_to_lemmatize: str, debug_mode_for_messages: bool = False) -> Set[str]:
    clean_word = word_to_lemmatize.strip().lower()
    if not clean_word: return set()
    analyses = morph.analyze(clean_word)
    possible_lemmas = set()
    for analysis_item in analyses:
        if analysis_item.get('analysis') and analysis_item['analysis']:
            lemma = analysis_item['analysis'][0]['lex'].lower()
            if lemma.strip().isalnum(): possible_lemmas.add(lemma)
        elif not possible_lemmas and 'text' in analysis_item:
            original_form = analysis_item['text'].lower()
            if original_form.strip().isalnum(): possible_lemmas.add(original_form)
    if debug_mode_for_messages and not possible_lemmas and clean_word:
        if 'debug_messages' not in st.session_state: st.session_state.debug_messages = []
        debug_msg_detail = f"–ê–Ω–∞–ª–∏–∑ Mystem: {str(analyses)[:100]}..." if analyses else "–ù–µ—Ç –∞–Ω–∞–ª–∏–∑–∞"
        st.session_state.debug_messages.append(f"[LSI Word (Cached)] –°–ª–æ–≤–æ '{clean_word}' –Ω–µ –¥–∞–ª–æ –ª–µ–º–º. {debug_msg_detail}")
    return possible_lemmas

def get_primary_lemmas_from_normalized_text(normalized_text: str, debug_mode: bool = False) -> Set[str]:
    if not normalized_text: return set()
    lemmas = morph.lemmatize(normalized_text)
    cleaned_lemmas = {token.lower() for token in lemmas if token.strip().isalnum()}
    if debug_mode:
        if 'debug_messages' not in st.session_state: st.session_state.debug_messages = []
        text_sample = normalized_text[:50] + "..." if len(normalized_text) > 50 else normalized_text
        st.session_state.debug_messages.append(f"[Page Primary Lemmas] –ò–∑ –Ω–æ—Ä–º. —Ç–µ–∫—Å—Ç–∞ ('{text_sample}', {len(normalized_text.split())} —Å–ª–æ–≤) "
                                               f"–ø–æ–ª—É—á–µ–Ω–æ {len(cleaned_lemmas)} —É–Ω–∏–∫. –æ—Å–Ω. –ª–µ–º–º. "
                                               f"–ü—Ä–∏–º–µ—Ä: {list(cleaned_lemmas)[:10] if cleaned_lemmas else '–ù–µ—Ç'}")
    return cleaned_lemmas

def get_stem_for_word(word: str) -> Optional[str]:
    if not RUSSIAN_STEMMER or not word or not word.strip(): return None
    return RUSSIAN_STEMMER.stemWord(word.lower().strip())

def check_exact_phrases(text: str, phrases_input: Optional[Any], debug_mode: bool = False) -> Dict[str, bool]:
    if not text or not phrases_input or (isinstance(phrases_input, float) and pd.isna(phrases_input)): return {}
    norm_page_text_for_exact_check = normalize_for_search(text)
    phrases_list = split_phrases(phrases_input)
    results = {}
    for phrase in phrases_list:
        norm_phrase_to_find = normalize_for_search(phrase)
        found = False
        if norm_phrase_to_find:
            found = norm_phrase_to_find in norm_page_text_for_exact_check
        if debug_mode:
            if 'debug_messages' not in st.session_state: st.session_state.debug_messages = []
            st.session_state.debug_messages.append({
                'type': 'exact_phrase_debug', 'phrase_original': phrase,
                'details': {'–æ—Ä–∏–≥–∏–Ω–∞–ª': phrase, '–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è_—Ñ—Ä–∞–∑–∞': norm_phrase_to_find,
                            '–≤—Ö–æ–∂–¥–µ–Ω–∏–µ_–≤_–Ω–æ—Ä–º_—Ç–µ–∫—Å—Ç (—Å—ç–º–ø–ª)': norm_page_text_for_exact_check[:100], '–Ω–∞–π–¥–µ–Ω–æ': found}
            })
        results[f"{PREFIX_EXACT_PHRASE}{phrase}"] = found
    return results

def check_lsi_phrases(raw_page_text: str, phrases_input: Optional[Any], debug_mode: bool = False) -> Dict[str, bool]:
    enable_truncation = st.session_state.get('enable_lsi_truncation', DEFAULT_ENABLE_LSI_TRUNCATION)
    trunc_max_remove = st.session_state.get('lsi_trunc_max_remove', DEFAULT_LSI_TRUNC_MAX_REMOVE)
    trunc_min_orig_len = st.session_state.get('lsi_trunc_min_orig_len', DEFAULT_LSI_TRUNC_MIN_ORIG_LEN)
    trunc_min_final_len = st.session_state.get('lsi_trunc_min_final_len', DEFAULT_LSI_TRUNC_MIN_FINAL_LEN)
    current_fuzzy_thresh = st.session_state.get('stem_fuzzy_ratio_threshold', DEFAULT_STEM_FUZZY_RATIO_THRESHOLD)

    if 'debug_messages' not in st.session_state: st.session_state.debug_messages = []

    if not raw_page_text or not phrases_input or (isinstance(phrases_input, float) and pd.isna(phrases_input)):
        if debug_mode: st.session_state.debug_messages.append("### LSI Debug: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö LSI –∏–ª–∏ —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—É—Å—Ç.")
        return {}

    if debug_mode:
        st.session_state.debug_messages.append(f"--- LSI: –í—Ö–æ–¥–Ω–æ–π raw_page_text (–ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤.): ---")
        st.session_state.debug_messages.append(raw_page_text[:300] + "...")
        st.session_state.debug_messages.append(f"(–û–±—â–∞—è –¥–ª–∏–Ω–∞ raw_page_text: {len(raw_page_text)})")

    normalized_page_content = normalize_for_search(raw_page_text)

    if debug_mode:
        st.session_state.debug_messages.append(f"--- LSI: –¢–µ–∫—Å—Ç –ü–û–°–õ–ï normalize_for_search (–ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤.): ---")
        st.session_state.debug_messages.append(normalized_page_content[:300] + "...")
        st.session_state.debug_messages.append(f"(–û–±—â–∞—è –¥–ª–∏–Ω–∞ normalized_page_content: {len(normalized_page_content)}, —Å–ª–æ–≤ –ø—Ä–∏–º–µ—Ä–Ω–æ: {len(normalized_page_content.split()) if normalized_page_content else 0})")

    if not normalized_page_content:
        if debug_mode: st.session_state.debug_messages.append("LSI Debug: –¢–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞–ª –ø—É—Å—Ç—ã–º –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏. LSI —Ñ—Ä–∞–∑—ã –Ω–µ –±—É–¥—É—Ç –Ω–∞–π–¥–µ–Ω—ã.")
        return {f"{PREFIX_LSI_PHRASE}{p.strip()}": False for p in split_phrases(phrases_input) if p.strip()}

    page_lemmas_set = get_primary_lemmas_from_normalized_text(normalized_page_content, debug_mode)
    page_stems_set = set()
    if RUSSIAN_STEMMER:
        for word_from_norm_page in normalized_page_content.split():
            stemmed_page_word = get_stem_for_word(word_from_norm_page)
            if stemmed_page_word: page_stems_set.add(stemmed_page_word)
        if debug_mode:
            if page_stems_set: st.session_state.debug_messages.append(f"[Page Stems] –ù–∞–π–¥–µ–Ω–æ {len(page_stems_set)} —É–Ω–∏–∫. –æ—Å–Ω–æ–≤. –ü—Ä–∏–º–µ—Ä: {list(page_stems_set)[:10]}")
            elif RUSSIAN_STEMMER: st.session_state.debug_messages.append("[Page Stems] –û—Å–Ω–æ–≤—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

    phrases_list = split_phrases(phrases_input)
    lsi_results = {}
    for phrase in phrases_list:
        if not phrase.strip(): continue
        normalized_lsi_phrase = normalize_for_search(phrase)
        lsi_phrase_words = [w for w in normalized_lsi_phrase.split() if w]
        if not lsi_phrase_words:
            lsi_results[f"{PREFIX_LSI_PHRASE}{phrase}"] = False
            if debug_mode: st.session_state.debug_messages.append({'LSI_—Ñ—Ä–∞–∑–∞': phrase, '—Å—Ç–∞—Ç—É—Å': '–§—Ä–∞–∑–∞ –ø—É—Å—Ç–∞ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏'})
            continue
        all_words_in_lsi_found = True
        debug_lsi_word_checks = []
        for word_in_lsi_phrase in lsi_phrase_words:
            found_this_word = False; match_type = "–Ω–µ—Ç"; matched_page_word_for_display = ""
            possible_lemmas_for_lsi_word = cached_lemmatize_word_flexibly(word_in_lsi_phrase, debug_mode)
            if possible_lemmas_for_lsi_word and any(lemma in page_lemmas_set for lemma in possible_lemmas_for_lsi_word):
                found_this_word = True; match_type = "–ª–µ–º–º–∞ (–≥–∏–±–∫–∞—è LSI -> –æ—Å–Ω–æ–≤–Ω–∞—è —Å—Ç—Ä.)"
            lsi_word_stem = None
            if not found_this_word and RUSSIAN_STEMMER:
                lsi_word_stem = get_stem_for_word(word_in_lsi_phrase)
                if lsi_word_stem:
                    if lsi_word_stem in page_stems_set: found_this_word = True; match_type = "–æ—Å–Ω–æ–≤–∞ (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)"
                    elif not found_this_word and RAPIDFUZZ_AVAILABLE and page_stems_set:
                        best_fuzzy_match_ratio = 0
                        for page_stem_candidate in page_stems_set:
                            ratio = fuzz.ratio(lsi_word_stem, page_stem_candidate)
                            if ratio >= current_fuzzy_thresh and ratio > best_fuzzy_match_ratio:
                                best_fuzzy_match_ratio = ratio; found_this_word = True
                        if found_this_word: match_type = f"–æ—Å–Ω–æ–≤–∞ (–Ω–µ—á–µ—Ç–∫–æ–µ, —Å—Ö–æ–∂–µ—Å—Ç—å {best_fuzzy_match_ratio:.0f}%)"
            truncation_info_for_debug = None
            if not found_this_word and enable_truncation:
                original_len = len(word_in_lsi_phrase)
                if original_len >= trunc_min_orig_len:
                    for chars_to_remove in range(1, trunc_max_remove + 1):
                        current_truncated_len = original_len - chars_to_remove
                        if current_truncated_len < trunc_min_final_len: break
                        truncated_lsi_word = word_in_lsi_phrase[:current_truncated_len]
                        try:
                            pattern = r'\b' + re.escape(truncated_lsi_word) + r'\w*\b'
                            match_object = re.search(pattern, normalized_page_content)
                            if match_object:
                                found_this_word = True; matched_page_word_for_display = match_object.group(0)
                                match_type = f"—É—Å–µ—á–µ–Ω–∏–µ –¥–æ '{truncated_lsi_word}' (–Ω–∞–π–¥–µ–Ω–æ: '{matched_page_word_for_display}')"
                                truncation_info_for_debug = {'truncated_to': truncated_lsi_word, 'matched_on_page': matched_page_word_for_display}; break
                        except re.error as e_re:
                            if debug_mode: st.session_state.debug_messages.append(f"–û—à–∏–±–∫–∞ Regex (—É—Å–µ—á–µ–Ω–∏–µ) –¥–ª—è '{truncated_lsi_word}': {e_re}")
                            continue
            if debug_mode:
                debug_entry = {'—Å–ª–æ–≤–æ_LSI_—Ñ—Ä–∞–∑—ã': word_in_lsi_phrase, '–ª–µ–º–º—ã_LSI_—Å–ª–æ–≤–∞': list(possible_lemmas_for_lsi_word),
                               '–æ—Å–Ω–æ–≤–∞_LSI_—Å–ª–æ–≤–∞': lsi_word_stem if lsi_word_stem else (get_stem_for_word(word_in_lsi_phrase) if RUSSIAN_STEMMER else "N/A"),
                               '–Ω–∞–π–¥–µ–Ω–æ': found_this_word, '—Ç–∏–ø_—Å–æ–≤–ø–∞–¥–µ–Ω–∏—è': match_type}
                if truncation_info_for_debug: debug_entry['–¥–µ—Ç–∞–ª–∏_—É—Å–µ—á–µ–Ω–∏—è'] = truncation_info_for_debug
                debug_lsi_word_checks.append(debug_entry)
            if not found_this_word: all_words_in_lsi_found = False; break
        lsi_results[f"{PREFIX_LSI_PHRASE}{phrase}"] = all_words_in_lsi_found
        if debug_mode:
            st.session_state.debug_messages.append({'type': 'lsi_phrase_full_debug', 'phrase_original': phrase,
                                                    'details': {'–æ—Ä–∏–≥–∏–Ω–∞–ª_LSI_—Ñ—Ä–∞–∑—ã': phrase, '–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è_LSI_—Ñ—Ä–∞–∑–∞': normalized_lsi_phrase,
                                                                '—Ü–µ–ª–µ–≤—ã–µ_—Å–ª–æ–≤–∞_—Ñ—Ä–∞–∑—ã': lsi_phrase_words, '–¥–µ—Ç–∞–ª–∏_–ø—Ä–æ–≤–µ—Ä–∫–∏_—Å–ª–æ–≤': debug_lsi_word_checks,
                                                                '–∏—Ç–æ–≥_—Ñ—Ä–∞–∑–∞_–Ω–∞–π–¥–µ–Ω–∞': all_words_in_lsi_found}})
    return lsi_results

@st.cache_data(ttl=3600)
def get_page_data_for_lang(base_ru_url: str, lang_to_fetch: str, debug_mode_internal: bool = False,
                           save_html_for_debug_manual: bool = False, filename_prefix_manual: str = "manual_debug_page") -> Dict[str, Any]:
    if 'debug_messages' not in st.session_state: st.session_state.debug_messages = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'ru-RU,ru;q=1.0,uk;q=0.8,en-US;q=0.6,en;q=0.4' if lang_to_fetch == 'ru' else 'uk-UA,uk;q=1.0,ru;q=0.8,en-US;q=0.6,en;q=0.4',
        'Connection': 'keep-alive', 'Upgrade-Insecure-Requests': '1', 'DNT': '1', 'Sec-GPC': '1',
    }
    session.cookies.set('language', lang_to_fetch, domain='apteka911.ua')
    session.cookies.set('lang', lang_to_fetch, domain='apteka911.ua')

    parsed_original_url = urlparse(base_ru_url)
    original_path = parsed_original_url.path.lstrip('/')
    original_query = parsed_original_url.query

    base_path = original_path
    if base_path.startswith('ua/'):
        base_path = base_path[3:]

    if lang_to_fetch == 'ua':
        final_path_processed = f"ua/{base_path}"
    else:
        final_path_processed = base_path

    base_modified_url = urljoin(BASE_URL_SITE, final_path_processed)
    modified_url_with_query = base_modified_url
    if original_query:
        if not base_modified_url.endswith('?'):
            modified_url_with_query = f"{base_modified_url}?{original_query}"
        else:
            modified_url_with_query = f"{base_modified_url}{original_query}"

    if debug_mode_internal:
        st.session_state.debug_messages.append(f"--- –û—Ç–ª–∞–¥–∫–∞ –¥–ª—è URL: {base_ru_url} (—è–∑—ã–∫: {lang_to_fetch}) ---")
        st.session_state.debug_messages.append(f"–ò—Å—Ö–æ–¥–Ω—ã–π URL (base): '{base_ru_url}'")
        st.session_state.debug_messages.append(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å: '{base_path}'")
        st.session_state.debug_messages.append(f"–°–æ–±—Ä–∞–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{modified_url_with_query}'")

    page_title, page_desc, page_full_text, error_message = "", "", "", None
    final_url_after_redirects = modified_url_with_query

    try:
        response = session.get(modified_url_with_query, headers=headers, timeout=20, verify=False, allow_redirects=True)
        response.raise_for_status()
        final_url_after_redirects = response.url
        if debug_mode_internal: st.session_state.debug_messages.append(f"[HTTP RESPONSE] Final URL: '{final_url_after_redirects}', Status: {response.status_code}, Apparent Encoding: {response.apparent_encoding}")

        response.encoding = 'utf-8'
        html_content = response.text

        if save_html_for_debug_manual:
            try:
                path_part = "".join(c if c.isalnum() else "_" for c in urlparse(final_url_after_redirects).path)
                safe_path_part = path_part.replace("__", "_")[:50]
                debug_html_filename = f"{filename_prefix_manual}_{lang_to_fetch}_{safe_path_part}.html"
                with open(debug_html_filename, "w", encoding="utf-8") as f: f.write(html_content)
                if debug_mode_internal: st.session_state.debug_messages.append(f"–û–¢–õ–ê–î–ö–ê (—Ä—É—á–Ω–∞—è): HTML –¥–ª—è {final_url_after_redirects} —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {debug_html_filename}")
            except Exception as e_save:
                if debug_mode_internal: st.session_state.debug_messages.append(f"–û–®–ò–ë–ö–ê –†–£–ß–ù–û–ì–û –°–û–•–†–ê–ù–ï–ù–ò–Ø HTML: {str(e_save)}")

    except requests.exceptions.RequestException as e:
        error_message = f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ {modified_url_with_query}: {str(e)}"
        if debug_mode_internal: st.session_state.debug_messages.append(f"[REQUEST ERROR] URL: {modified_url_with_query}, Error: {error_message}")
        return {'title': '', 'description': '', 'full_text': '', 'error': error_message, 'final_url_fetched': modified_url_with_query}

    soup = BeautifulSoup(html_content, 'html.parser')
    title_tag = soup.find('title')
    description_tag = soup.find('meta', attrs={'name': 'description'})
    page_title = title_tag.text.strip() if title_tag else ''
    page_desc = description_tag['content'].strip() if description_tag and description_tag.get('content') else ''

    if debug_mode_internal:
        st.session_state.debug_messages.append(f"[META EXTRACTED] –ò–∑ URL: '{final_url_after_redirects}', Title Found: {'–î–∞' if page_title else '–ù–µ—Ç'}, Title: '{page_title[:150]}...'")
        st.session_state.debug_messages.append(f"[META EXTRACTED] –ò–∑ URL: '{final_url_after_redirects}', Desc Found: {'–î–∞' if page_desc else '–ù–µ—Ç'}, Desc: '{page_desc[:150]}...'")

    content_parts = []
    main_content_selectors = ['article', 'main', '.main-content', '.content', '.post-content', '.entry-content', '#content', '.b-content__body', '.js-mediator-article', '.product-description', '.page-content']
    content_area = None
    for selector in main_content_selectors:
        if selector.startswith('.'): content_area = soup.find(class_=selector[1:])
        elif selector.startswith('#'): content_area = soup.find(id=selector[1:])
        else: content_area = soup.find(selector)
        if content_area:
            if debug_mode_internal and 'debug_messages' in st.session_state: st.session_state.debug_messages.append(f"[CONTENT AREA] –ù–∞–π–¥–µ–Ω –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É: '{selector}' –¥–ª—è URL {final_url_after_redirects}")
            break
    if not content_area and debug_mode_internal and 'debug_messages' in st.session_state: st.session_state.debug_messages.append(f"[CONTENT AREA] –û—Å–Ω–æ–≤–Ω–æ–π –±–ª–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è {final_url_after_redirects}, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–µ—Å—å 'soup'.")

    source_tags_container = content_area if content_area else soup
    source_tags = source_tags_container.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'span', 'td', 'th', 'strong', 'em', 'b', 'i', 'div'])
    noisy_classes = ['advert', 'social', 'comment', 'sidebar', 'menu', 'nav', 'footer', 'header', 'modal', 'popup', 'banner', 'widget', 'related-posts', 'author-bio', 'breadcrumb', 'pagination', 'meta', 'hidden', 'sr-only', 'price', 'tools', 'actions', 'rating', 'tags', 'share', 'author', 'date', 'category', 'edit-link', 'reply', 'navigation', 'top-link', 'skip-link', 'visually-hidden', 'cookie', 'alert', 'dropdown', 'tab']
    noisy_ids = ['comments', 'sidebar', 'navigation', 'footer', 'header', 'modal', 'popup', 'respond', 'author-info', 'related', 'sharing', 'secondary', 'primary-menu', 'top-bar', 'cookie-banner', 'gdpr-consent']
    noisy_tags_in_parents = ['script', 'style', 'nav', 'footer', 'aside', 'header', 'form', 'button', 'select', 'textarea', 'iframe', 'noscript', 'svg', 'figure', 'figcaption', 'address']
    for tag in source_tags:
        is_noisy = False
        if tag.name == 'div' and not tag.find(['p','li','span','h1','h2','h3','h4','h5','h6'], recursive=False) and len(tag.get_text(strip=True)) < 50: is_noisy = True
        if not is_noisy and tag.has_attr('class') and any(cls in noisy_classes for cls in tag.get('class', [])): is_noisy = True
        if not is_noisy and tag.has_attr('id') and any(id_val in noisy_ids for id_val in tag.get('id', [])): is_noisy = True
        if not is_noisy:
            for parent in tag.parents:
                if parent.name in noisy_tags_in_parents: is_noisy = True; break
                if parent.has_attr('class') and any(cls in noisy_classes for cls in parent.get('class', [])): is_noisy = True; break
                if parent.has_attr('id') and any(id_val in noisy_ids for id_val in parent.get('id', [])): is_noisy = True; break
        if not is_noisy:
            tag_text = tag.get_text(separator=' ', strip=True)
            if tag.name in ['span','div'] and len(tag_text.split()) < 3 and not any(c.isdigit() for c in tag_text):
                if len(tag_text) < 15: continue
            if tag_text: content_parts.append(tag_text)
    content = ' '.join(filter(None, content_parts))
    if debug_mode_internal and 'debug_messages' in st.session_state:
        st.session_state.debug_messages.append(f"[CONTENT SAMPLE for {final_url_after_redirects}] '{content[:300]}...' (–í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {len(content)})")
    page_full_text = f"{page_title} {page_desc} {content}"
    return {'title': page_title, 'description': page_desc, 'full_text': page_full_text, 'error': error_message, 'final_url_fetched': final_url_after_redirects}

@st.cache_data
def load_all_pages_data_for_both_langs(dataframe: pd.DataFrame, _debug_mode_global: bool) -> Dict[str, Dict[str, Dict[str, Any]]]:
    all_data = {}
    total_rows = len(dataframe)
    if 'global_progress_text' not in st.session_state: st.session_state.global_progress_text = ""
    if 'global_progress_value' not in st.session_state: st.session_state.global_progress_value = 0.0
    for i, row in dataframe.iterrows():
        base_ru_url = str(row[COL_URL_RU_EXCEL])
        all_data[base_ru_url] = {}
        for lang_idx, lang_code in enumerate(['ru', 'ua']):
            current_op_total = (i * 2) + (lang_idx + 1)
            total_ops_overall = total_rows * 2
            st.session_state.global_progress_text = f"–ó–∞–≥—Ä—É–∑–∫–∞: {current_op_total}/{total_ops_overall} ({base_ru_url} - {lang_code.upper()})"
            st.session_state.global_progress_value = current_op_total / total_ops_overall
            if _debug_mode_global: print(st.session_state.global_progress_text)
            manual_debug_url_val = st.session_state.get("manual_debug_url_val", None)
            manual_debug_lang_val = st.session_state.get("manual_debug_lang_val", None)
            save_html_this_iteration = _debug_mode_global and base_ru_url == manual_debug_url_val and lang_code == manual_debug_lang_val
            all_data[base_ru_url][lang_code] = get_page_data_for_lang(base_ru_url, lang_code,
                                                                      debug_mode_internal=_debug_mode_global,
                                                                      save_html_for_debug_manual=save_html_this_iteration,
                                                                      filename_prefix_manual="auto_save_on_load")
    st.session_state.global_progress_text = "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞!"
    st.session_state.global_progress_value = 1.0
    if _debug_mode_global: print(st.session_state.global_progress_text)
    return all_data

def display_debug_messages():
    if 'debug_messages' in st.session_state and st.session_state.debug_messages:
        st.sidebar.markdown("--- –û—Ç–ª–∞–¥–æ—á–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è ---")
        for msg_item in reversed(st.session_state.debug_messages):
            if isinstance(msg_item, dict) and 'type' in msg_item:
                msg_type = msg_item['type']
                phrase = msg_item.get('phrase_original', msg_item.get('phrase', 'N/A'))
                details = msg_item.get('details', msg_item)
                if msg_type == 'exact_phrase_debug':
                    status_icon = "‚úÖ" if details.get('–Ω–∞–π–¥–µ–Ω–æ') else "‚ùå"
                    st.sidebar.markdown(f"##### {status_icon} –¢–æ—á–Ω–∞—è: '{details.get('–æ—Ä–∏–≥–∏–Ω–∞–ª', phrase)}'")
                    st.sidebar.json({'–ù–æ—Ä–º–∞–ª–∏–∑.': details.get('–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è_—Ñ—Ä–∞–∑–∞'), '–ù–∞–π–¥–µ–Ω–æ': details.get('–Ω–∞–π–¥–µ–Ω–æ')})
                elif msg_type == 'lsi_phrase_full_debug':
                    status_icon = "‚úÖ" if details.get('–∏—Ç–æ–≥_—Ñ—Ä–∞–∑–∞_–Ω–∞–π–¥–µ–Ω–∞') else "‚ùå"
                    st.sidebar.markdown(f"##### {status_icon} LSI: '{phrase}'")
                    st.sidebar.json(details)
                else:
                    st.sidebar.write(f"Debug Item (type: {msg_type}):"); st.sidebar.json(msg_item)
            elif isinstance(msg_item, str): st.sidebar.markdown(msg_item)
            else: st.sidebar.write(msg_item)
    if st.sidebar.button("–û—á–∏—Å—Ç–∏—Ç—å –ª–æ–≥ –æ—Ç–ª–∞–¥–∫–∏", key="clear_debug_button_sidebar"):
        st.session_state.debug_messages = []
        st.rerun()

def run_checks_for_language(lang_to_check: str, df_excel: pd.DataFrame,
                            all_site_data: Dict[str, Dict[str, Dict[str, Any]]],
                            debug_mode: bool):
    if lang_to_check == 'ua':
        expected_title_col = COL_TITLE_UA_EXCEL; expected_desc_col = COL_DESC_UA_EXCEL
        exact_phrases_col = COL_EXACT_PHRASES_UA_EXCEL; lsi_col = COL_LSI_UA_EXCEL
        if expected_title_col not in df_excel.columns and COL_TITLE_RU_EXCEL in df_excel.columns:
            if debug_mode and 'debug_messages' in st.session_state: st.session_state.debug_messages.append(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï (UA): –ö–æ–ª. Title '{COL_TITLE_UA_EXCEL}' –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É—é '{COL_TITLE_RU_EXCEL}'.")
            expected_title_col = COL_TITLE_RU_EXCEL
        if expected_desc_col not in df_excel.columns and COL_DESC_RU_EXCEL in df_excel.columns:
            if debug_mode and 'debug_messages' in st.session_state: st.session_state.debug_messages.append(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï (UA): –ö–æ–ª. Desc '{COL_DESC_UA_EXCEL}' –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É—é '{COL_DESC_RU_EXCEL}'.")
            expected_desc_col = COL_DESC_RU_EXCEL
    else:
        expected_title_col = COL_TITLE_RU_EXCEL; expected_desc_col = COL_DESC_RU_EXCEL
        exact_phrases_col = COL_EXACT_PHRASES_RU_EXCEL; lsi_col = COL_LSI_RU_EXCEL
    required_cols_for_run = [COL_URL_RU_EXCEL, expected_title_col, expected_desc_col]
    missing_cols_in_df = [col for col in required_cols_for_run if col not in df_excel.columns]
    if missing_cols_in_df:
        st.error(f"–î–ª—è —è–∑—ã–∫–∞ {lang_to_check.upper()}: –í Excel –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤: {', '.join(missing_cols_in_df)}. "
                 f"–û–∂–∏–¥–∞–ª–∏—Å—å: {', '.join(required_cols_for_run)}")
        return
    sub_tab_meta, sub_tab_phrases = st.tabs([f"üìã –û–±—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ Title/Description", f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñraz"])
    with sub_tab_meta:
        tab1_errors_summary = {'load_error': 0, 'title_mismatch': 0, 'desc_mismatch': 0}
        tab1_processed_rows_data, urls_with_meta_issues_list_tab1 = [], []
        for index, row_from_df in df_excel.iterrows():
            base_url_from_row = str(row_from_df[COL_URL_RU_EXCEL])
            page_lang_specific_data = all_site_data.get(base_url_from_row, {}).get(lang_to_check, {})
            item_details = {'url': base_url_from_row, 'final_url': page_lang_specific_data.get('final_url_fetched', base_url_from_row), 'has_issue': False, 'issue_details': []}
            if page_lang_specific_data.get('error'):
                tab1_errors_summary['load_error'] += 1; item_details['has_issue'] = True
                item_details['issue_details'].append(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {page_lang_specific_data['error']}")
            else:
                expected_title = str(row_from_df.get(expected_title_col, "")).strip()
                expected_desc = str(row_from_df.get(expected_desc_col, "")).strip()
                site_title = page_lang_specific_data.get('title', "").strip()
                site_desc = page_lang_specific_data.get('description', "").strip()
                title_match = normalize_for_search(site_title) == normalize_for_search(expected_title) if expected_title else (not site_title)
                desc_match = normalize_for_search(site_desc) == normalize_for_search(expected_desc) if expected_desc else (not site_desc)
                item_details.update({'expected_title': expected_title, 'expected_desc': expected_desc, 'site_title': site_title, 'site_desc': site_desc, 'title_match': title_match, 'desc_match': desc_match})
                if not title_match: tab1_errors_summary['title_mismatch'] += 1; item_details['has_issue'] = True; item_details['issue_details'].append(f'Title –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç')
                if not desc_match: tab1_errors_summary['desc_mismatch'] += 1; item_details['has_issue'] = True; item_details['issue_details'].append(f'Desc –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç')
            if item_details['has_issue']: urls_with_meta_issues_list_tab1.append(item_details['final_url'])
            tab1_processed_rows_data.append(item_details)
        st.info(f"–û—à–∏–±–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏: {tab1_errors_summary['load_error']} | –ù–µ—Å–æ–≤–ø. Title: {tab1_errors_summary['title_mismatch']} | –ù–µ—Å–æ–≤–ø. Desc: {tab1_errors_summary['desc_mismatch']}")
        show_only_meta_errors_cb = st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ª—å–∫–æ URL —Å –æ—à–∏–±–∫–∞–º–∏", value=True, key=f"show_err_meta_{lang_to_check}")
        for item_m in tab1_processed_rows_data:
            if show_only_meta_errors_cb and not item_m['has_issue']: continue
            exp_icon = "‚úÖ" if not item_m['has_issue'] else "‚ö†Ô∏è"
            exp_title = f"{exp_icon} {item_m['final_url']}" + (f" ({', '.join(item_m['issue_details'])})" if item_m['has_issue'] else "")
            with st.expander(exp_title, expanded=item_m['has_issue']):
                if "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏" in "".join(item_m['issue_details']): st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. {item_m['issue_details'][0]}")
                else:
                    st.markdown(f"""<div class="info-box"><b>Title:</b> {'‚úÖ –°–æ–≤–ø–∞–¥–∞–µ—Ç' if item_m['title_match'] else '‚ùå –ù–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç'}
                                <div class="result-content"><b>–û–∂–∏–¥–∞–ª–æ—Å—å ({expected_title_col}):</b> {html.escape(str(item_m['expected_title']))}</div>
                                <div class="result-content"><b>–ù–∞ —Å–∞–π—Ç–µ:</b> {html.escape(str(item_m['site_title']))}</div></div>
                                <div class="info-box"><b>Description:</b> {'‚úÖ –°–æ–≤–ø–∞–¥–∞–µ—Ç' if item_m['desc_match'] else '‚ùå –ù–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç'}
                                <div class="result-content"><b>–û–∂–∏–¥–∞–ª–æ—Å—å ({expected_desc_col}):</b> {html.escape(str(item_m['expected_desc']))}</div>
                                <div class="result-content"><b>–ù–∞ —Å–∞–π—Ç–µ:</b> {html.escape(str(item_m['site_desc']))}</div></div>""", unsafe_allow_html=True)
    with sub_tab_phrases:
        prog_bar_phrases = st.progress(0)
        stat_text_phrases = st.empty()
        exact_col_exists = exact_phrases_col in df_excel.columns
        lsi_col_exists = lsi_col in df_excel.columns
        if not exact_col_exists and debug_mode and 'debug_messages' in st.session_state: st.session_state.debug_messages.append(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï ({lang_to_check.upper()}): –ö–æ–ª. —Ç–æ—á–Ω—ã—Ö —Ñ—Ä–∞–∑ '{exact_phrases_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        if not lsi_col_exists and debug_mode and 'debug_messages' in st.session_state: st.session_state.debug_messages.append(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï ({lang_to_check.upper()}): –ö–æ–ª. LSI '{lsi_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        phrase_results_list = []
        for idx, df_row in df_excel.iterrows():
            base_url_from_row = str(df_row[COL_URL_RU_EXCEL])
            stat_text_phrases.info(f"–ê–Ω–∞–ª–∏–∑ —Ñ—Ä–∞–∑ –¥–ª—è URL {idx + 1}/{len(df_excel)}: {base_url_from_row}")
            prog_bar_phrases.progress((idx + 1) / len(df_excel))
            lang_data = all_site_data.get(base_url_from_row, {}).get(lang_to_check, {})

            final_url_for_display = lang_data.get('final_url_fetched', base_url_from_row)

            if lang_data.get('error'):
                phrase_results_list.append({"URL": final_url_for_display, "–¢–∏–ø —Ñ—Ä–∞–∑—ã": "N/A", "–§—Ä–∞–∑–∞": "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", "–°—Ç–∞—Ç—É—Å": "‚ùå –û—à–∏–±–∫–∞"})
                if debug_mode and 'debug_messages' in st.session_state: st.session_state.debug_messages.append(f"Tab2 ({lang_to_check.upper()}): –ü—Ä–æ–ø—É—Å–∫ —Ñ—Ä–∞–∑ {base_url_from_row}, –æ—à–∏–±–∫–∞: {lang_data.get('error','') if lang_data else ''}")
                continue

            full_text = lang_data.get('full_text', "")
            if not full_text and debug_mode and 'debug_messages' in st.session_state: st.session_state.debug_messages.append(f"Tab2 ({lang_to_check.upper()}): –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è {final_url_for_display}")

            current_url_phrases = {}
            if exact_col_exists and pd.notna(df_row.get(exact_phrases_col)):
                current_url_phrases.update(check_exact_phrases(full_text, df_row[exact_phrases_col], debug_mode))
            if lsi_col_exists and pd.notna(df_row.get(lsi_col)):
                if debug_mode and 'debug_messages' in st.session_state:
                    st.session_state.debug_messages.append(f"--- LSI –¥–ª—è URL: {final_url_for_display} ({lang_to_check.upper()}) (Fuzzy: {st.session_state.get('stem_fuzzy_ratio_threshold', DEFAULT_STEM_FUZZY_RATIO_THRESHOLD)}%) ---")
                    st.session_state.debug_messages.append(f"LSI –∏–∑ '{lsi_col}': '{df_row.get(lsi_col)}'")
                current_url_phrases.update(check_lsi_phrases(full_text, df_row[lsi_col], debug_mode))

            for phrase_key, found in current_url_phrases.items():
                p_type = "–¢–æ—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ" if phrase_key.startswith(PREFIX_EXACT_PHRASE) else "LSI —Ñ—Ä–∞–∑–∞"
                act_phr = phrase_key.replace(PREFIX_EXACT_PHRASE, "").replace(PREFIX_LSI_PHRASE, "")
                phrase_results_list.append({"URL": final_url_for_display, "–¢–∏–ø —Ñ—Ä–∞–∑—ã": p_type, "–§—Ä–∞–∑–∞": act_phr, "–°—Ç–∞—Ç—É—Å": "‚úÖ –ù–∞–π–¥–µ–Ω–æ" if found else "‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ"})

        stat_text_phrases.success(f"–ê–Ω–∞–ª–∏–∑ —Ñ—Ä–∞–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        prog_bar_phrases.empty()

        if phrase_results_list:
            phrases_df = pd.DataFrame(phrase_results_list)
            urls_w_failed_phr = phrases_df[phrases_df['–°—Ç–∞—Ç—É—Å'].str.contains("‚ùå")]['URL'].nunique()
            total_phr_not_found = len(phrases_df[phrases_df['–°—Ç–∞—Ç—É—Å'] == "‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ"])
            st.info(f"URL —Å –Ω–µ–Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏/–æ—à–∏–±–æ—á–Ω—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏: {urls_w_failed_phr} | –í—Å–µ–≥–æ —Ñ—Ä–∞–∑ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {total_phr_not_found}")

            fcols = st.columns(3)
            unique_urls = sorted(phrases_df["URL"].unique().tolist()) if not phrases_df.empty else []
            unique_types = sorted(phrases_df["–¢–∏–ø —Ñ—Ä–∞–∑—ã"].unique().tolist()) if not phrases_df.empty else []
            unique_statuses = sorted(phrases_df["–°—Ç–∞—Ç—É—Å"].unique().tolist()) if not phrases_df.empty else []
            with fcols[0]: url_f = st.multiselect("URL", unique_urls, key=f"url_f_t2_multi_{lang_to_check}")
            with fcols[1]: ptype_f = st.multiselect("–¢–∏–ø —Ñ—Ä–∞–∑—ã", unique_types, key=f"ptype_f_t2_multi_{lang_to_check}")
            with fcols[2]: stat_f = st.selectbox("–°—Ç–∞—Ç—É—Å", ["–í—Å–µ —Å—Ç–∞—Ç—É—Å—ã"] + unique_statuses, key=f"stat_f_t2_select_{lang_to_check}")

            filtered_df = phrases_df.copy()
            if url_f: filtered_df = filtered_df[filtered_df["URL"].isin(url_f)]
            if ptype_f: filtered_df = filtered_df[filtered_df["–¢–∏–ø —Ñ—Ä–∞–∑—ã"].isin(ptype_f)]
            if stat_f != "–í—Å–µ —Å—Ç–∞—Ç—É—Å—ã": filtered_df = filtered_df[filtered_df["–°—Ç–∞—Ç—É—Å"] == stat_f]

            st.dataframe(filtered_df, hide_index=True, use_container_width=True, height=600,
                         column_config={"URL": st.column_config.TextColumn("–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π URL", width="medium"),
                                        "–§—Ä–∞–∑–∞": st.column_config.TextColumn("–§—Ä–∞–∑–∞", width="large")})
            csv_dl = filtered_df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(f"üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ({lang_to_check.upper()})", csv_dl, f'filtered_phrases_{lang_to_check}.csv', 'text/csv', key=f"dl_phr_t2_btn_{lang_to_check}")
        else: st.warning(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ñ—Ä–∞–∑–∞–º –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")

# ========== –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ==========
def main():
    st.sidebar.title("üåü –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã SEO-–∫–æ–º–±–∞–π–Ω–∞")
    tab = st.sidebar.radio(
        "–í—ã–±–µ—Ä–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç:",
        [
            "üîé –ü–æ–¥–±–æ—Ä –∫–ª—é—á–µ–π (Serpstat)",
            "üßô –ê–Ω–∞–ª–∏–∑ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤",
            "üîç –ü–æ–¥—Å–≤–µ—Ç–∫–∞ —Å–ª–æ–≤ + DOCX",
            "üîç SEO Meta Checker",
            "üß™ Tittle_Description +",
            "üìñ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è"
        ],
        index=0
    )
    # ========== –í–ö–õ–ê–î–ö–ê 6: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è ==========
    if tab == "üìñ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è":
        st.title("üìñ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ SEO-–∫–æ–º–±–∞–π–Ω–∞")
        st.markdown("""
        ### üîé –ü–æ–¥–±–æ—Ä –∫–ª—é—á–µ–π (Serpstat)
        - –ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel-—Ñ–∞–π–ª —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: **–ù–∞–∑–≤–∞–Ω–∏–µ**, **URL**, **–§—Ä–∞–∑—ã –≤ —Ç–æ—á–Ω–æ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–∏**.
        - –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –∑–∞–ø—É—Å–∫–∞. –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–æ–±—Ä–∞–Ω —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ —á–µ—Ä–µ–∑ Serpstat API.
        - –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–∫–∞—á–∞–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –Ω–æ–≤—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏.

        ---
        ### üßô –ê–Ω–∞–ª–∏–∑ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
        - –í—Å—Ç–∞–≤—å—Ç–µ –¥–≤–∞ —Ç–µ–∫—Å—Ç–∞: –ø–µ—Ä–≤—ã–π ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π, –≤—Ç–æ—Ä–æ–π ‚Äî —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º—ã–π.
        - –ù–∞–∂–º–∏—Ç–µ "–ù–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑". –ë—É–¥—É—Ç –Ω–∞–π–¥–µ–Ω—ã —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤–æ –≤—Ç–æ—Ä–æ–º —Ç–µ–∫—Å—Ç–µ, –Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –ø–µ—Ä–≤–æ–º (—Å —É—á—ë—Ç–æ–º –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏).

        ---
        ### üîç –ü–æ–¥—Å–≤–µ—Ç–∫–∞ —Å–ª–æ–≤ + DOCX
        - –í–≤–µ–¥–∏—Ç–µ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–µ–∫—Å—Ç, –∞ —Ç–∞–∫–∂–µ —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É).
        - –ù–∞–∂–º–∏—Ç–µ "–ü–æ–¥—Å–≤–µ—Ç–∏—Ç—å –∏ –ø–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç" –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤—ã–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–≤.
        - –ú–æ–∂–Ω–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ Word (DOCX).

        ---
        ### üîç SEO Meta Checker
        - –ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel-—Ñ–∞–π–ª —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: **–¶–µ–ª–µ–≤–æ–π URL (RU)**, **Title RU**, **Description RU**, **Title UA**, **Description UA**, **–§—Ä–∞–∑—ã –≤ —Ç–æ—á–Ω–æ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–∏ RU/UA**, **LSI/LSI UA**.
        - –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤ –Ω–∞ —Å–∞–π—Ç–µ –∏ –Ω–∞–ª–∏—á–∏–µ —Ñ—Ä–∞–∑ (—Ç–æ—á–Ω—ã—Ö –∏ LSI) –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        - –î–æ—Å—Ç—É–ø–Ω–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è, –ø—Ä–æ—Å–º–æ—Ç—Ä –æ—à–∏–±–æ–∫ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

        ---
        ### üß™ Tittle_Description +
        - –ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel-—Ñ–∞–π–ª —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: **URL**, **Title RU**, **Description RU**, **Title UA**, **Description UA**.
        - –î–ª—è –∫–∞–∂–¥–æ–≥–æ URL –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –º–µ—Ç–∞-—Ç–µ–≥–∏ —Å–∞–π—Ç–∞ —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è).
        - –ú–æ–∂–Ω–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏ —Å–∫–∞—á–∞—Ç—å –∏—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª.

        ---
        **–ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–ª–∏ –≤–æ–ø—Ä–æ—Å—ã ‚Äî —Å–º. –ø–æ–¥—Å–∫–∞–∑–∫–∏ –≤ –∫–∞–∂–¥–æ–π –≤–∫–ª–∞–¥–∫–µ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫—É.**
        """)
        st.info("–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–∞ –¥–ª—è –≤—Å–µ—Ö –≤–∫–ª–∞–¥–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è. –î–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π —Å–º. –æ–ø–∏—Å–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π –≤–∫–ª–∞–¥–∫–∏.")
        return

    # ========== –í–ö–õ–ê–î–ö–ê 5: SEO –ú–µ—Ç–∞-–ü—Ä–æ–≤–µ—Ä–∫–∞ 2.0 ==========
    if tab == "üß™ Tittle_Description +":
        import tempfile
        import altair as alt
        from difflib import SequenceMatcher

        st.set_page_config(page_title="SEO –ú–µ—Ç–∞-–ü—Ä–æ–≤–µ—Ä–∫–∞ Apteka 9-1-1", layout="wide")
        st.title("üîç SEO –ú–µ—Ç–∞-–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è —Å–∞–π—Ç–∞ Apteka 9-1-1")

        st.markdown("""
        <style>
            .result-box {
                border: 1px solid #ccc;
                border-radius: 10px;
                padding: 15px;
                margin-bottom: 10px;
                background-color: #f9f9f9;
            }
            .highlight {
                font-weight: bold;
                color: #2c3e50;
            }
            .progress-bar {
                height: 20px;
                background-color: #f3f3f3;
                border-radius: 10px;
                margin: 10px 0;
            }
            .progress {
                height: 100%;
                background-color: #66bb6a;
                border-radius: 10px;
                transition: width 0.3s ease;
            }
            .tooltip {
                position: relative;
                display: inline-block;
                cursor: pointer;
            }
            .tooltip .tooltiptext {
                visibility: hidden;
                width: 200px;
                background-color: #555;
                color: #fff;
                text-align: center;
                padding: 5px;
                border-radius: 6px;
                position: absolute;
                z-index: 1;
                bottom: 125%;
                left: 50%;
                margin-left: -100px;
                opacity: 0;
                transition: opacity 0.3s;
            }
            .tooltip:hover .tooltiptext {
                visibility: visible;
                opacity: 1;
            }
            .stat-card {
                border: 1px solid #ddd;
                border-radius: 8px;
                padding: 15px;
                margin: 10px;
                background: white;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }
        </style>
        """, unsafe_allow_html=True)

        def clean_text(text):
            text = str(text).lower()
            text = re.sub(r'%min_price%', '', text)
            text = re.sub(r'—Ü–µ–Ω–∞ –æ—Ç [^|\n\r-]+', '', text)
            text = re.sub(r'—Üi–Ω–∞ –≤i–¥ [^|\n\r-]+', '', text)
            text = re.sub(r'[\-\|:,‚≠ê‚è©‚ö°üîπüì¶‚Üí¬Æ]', '', text)
            text = re.sub(r'–≥—Ä–Ω|uah', '', text)
            text = re.sub(r'\s+', ' ', text)
            return text.strip()

        def get_similarity(text1, text2):
            return round(SequenceMatcher(None, text1, text2).ratio() * 100, 1)

        def get_meta_from_url(url):
            try:
                headers = {'User-Agent': 'Mozilla/5.0'}
                response = requests.get(url, headers=headers, timeout=10)
                soup = BeautifulSoup(response.text, 'html.parser')
                title = soup.title.string.strip() if soup.title else ''
                description = ''
                tag = soup.find("meta", attrs={"name": "description"})
                if tag and tag.get("content"):
                    description = tag["content"].strip()
                return title, description
            except:
                return '', ''

        tabs2 = st.tabs(["–ó–∞–≥—Ä—É–∑–∫–∞", "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã"])

        with tabs2[0]:
            st.subheader("üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
            st.markdown("""
            <div class="tooltip">
                –ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel-—Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏
                <span class="tooltiptext">
                    –§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: URL, Title RU, Description RU, Title UA, Description UA
                </span>
            </div>
            """, unsafe_allow_html=True)
            uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel-—Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏:", type=["xlsx"], key="meta2_file")
            if 'meta2_result_df' not in st.session_state:
                st.session_state.meta2_result_df = None

            if uploaded_file:
                df = pd.read_excel(uploaded_file)
                st.success("–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ. –ù–∞—á–∏–Ω–∞—é –ø—Ä–æ–≤–µ—Ä–∫—É...")
                progress_bar = st.progress(0)
                status_text = st.empty()
                results = []
                total = len(df)
                for index, row in df.iterrows():
                    progress = (index + 1) / total
                    progress_bar.progress(progress)
                    status_text.text(f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ {index + 1}/{total} URL")
                    url = str(row['URL']).strip()
                    url_ua = url.replace("apteka911.ua/", "apteka911.ua/ua/")
                    row_result = {
                        'URL': url,
                        'Title RU (—Ç–∞–±–ª–∏—Ü–∞)': row.get('Title RU', ''),
                        'Description RU (—Ç–∞–±–ª–∏—Ü–∞)': row.get('Description RU', ''),
                        'Title UA (—Ç–∞–±–ª–∏—Ü–∞)': row.get('Title UA', ''),
                        'Description UA (—Ç–∞–±–ª–∏—Ü–∞)': row.get('Description UA', '')
                    }
                    # RU
                    title_ru_site, desc_ru_site = get_meta_from_url(url)
                    row_result['Title RU (—Å–∞–π—Ç)'] = title_ru_site
                    row_result['Description RU (—Å–∞–π—Ç)'] = desc_ru_site
                    # UA
                    title_ua_site, desc_ua_site = get_meta_from_url(url_ua)
                    row_result['Title UA (—Å–∞–π—Ç)'] = title_ua_site
                    row_result['Description UA (—Å–∞–π—Ç)'] = desc_ua_site
                    # –°—Ö–æ–¥—Å—Ç–≤–æ
                    title_ru_sim = get_similarity(clean_text(row.get('Title RU', '')), clean_text(title_ru_site))
                    desc_ru_sim = get_similarity(clean_text(row.get('Description RU', '')), clean_text(desc_ru_site))
                    title_ua_sim = get_similarity(clean_text(row.get('Title UA', '')), clean_text(title_ua_site))
                    desc_ua_sim = get_similarity(clean_text(row.get('Description UA', '')), clean_text(desc_ua_site))
                    row_result['Title RU –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ (%)'] = title_ru_sim
                    row_result['Description RU –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ (%)'] = desc_ru_sim
                    row_result['Title UA –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ (%)'] = title_ua_sim
                    row_result['Description UA –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ (%)'] = desc_ua_sim
                    results.append(row_result)
                status_text.text("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
                result_df = pd.DataFrame(results)
                st.session_state.meta2_result_df = result_df
                def highlight_percentage(p):
                    if p >= 80:
                        return f"‚úÖ {p}%"
                    elif p >= 50:
                        return f"üü° {p}%"
                    else:
                        return f"‚ùå {p}%"
                display_df = result_df.copy()
                for col in display_df.columns:
                    if "–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ" in col:
                        display_df[col] = display_df[col].apply(highlight_percentage)
                with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
                    result_df.to_excel(tmp.name, index=False)
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç", data=open(tmp.name, "rb"), file_name="seo_meta_check_result.xlsx")

        with tabs2[1]:
            st.subheader("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏")
            result_df = st.session_state.get('meta2_result_df', None)
            if result_df is not None:
                min_similarity = st.slider("–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è", 0, 100, 50)
                filtered_df = result_df.copy()
                filtered_df = filtered_df[(filtered_df['Title RU –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ (%)'] >= min_similarity) |
                                        (filtered_df['Description RU –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ (%)'] >= min_similarity) |
                                        (filtered_df['Title UA –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ (%)'] >= min_similarity) |
                                        (filtered_df['Description UA –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ (%)'] >= min_similarity)]
                def highlight_percentage(p):
                    if p >= 80:
                        return f"‚úÖ {p}%"
                    elif p >= 50:
                        return f"üü° {p}%"
                    else:
                        return f"‚ùå {p}%"
                display_df = filtered_df.copy()
                for col in display_df.columns:
                    if "–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ" in col:
                        display_df[col] = display_df[col].apply(highlight_percentage)
                st.dataframe(display_df, use_container_width=True)
            else:
                st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏")

    st.sidebar.markdown("---")
    st.sidebar.info("–°–¥–µ–ª–∞–Ω–æ —Å –∑–∞–±–æ—Ç–æ–π –æ —Ç–≤–æ—ë–º –≤—Ä–µ–º–µ–Ω–∏ ‚ù§Ô∏è‚Äçüî•", icon="üí°")

    # ========== –í–ö–õ–ê–î–ö–ê 1: –ü–æ–¥–±–æ—Ä –∫–ª—é—á–µ–π (Serpstat) ==========
    if tab == "üîé –ü–æ–¥–±–æ—Ä –∫–ª—é—á–µ–π (Serpstat)":
        st.title("üîé –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥–±–æ—Ä –∫–ª—é—á–µ–π –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞")

        uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏ Excel-—Ñ–∞–π–ª (.xlsx) —Å –ù–∞–∑–≤–∞–Ω–∏–µ–º, URL –∏ –§—Ä–∞–∑—ã", type=["xlsx"])
        start_button = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É")

        if uploaded_file and start_button:
            with st.spinner("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª..."):
                df = pd.read_excel(uploaded_file)
                if "–ù–∞–∑–≤–∞–Ω–∏–µ" not in df.columns or "URL" not in df.columns or "–§—Ä–∞–∑—ã –≤ —Ç–æ—á–Ω–æ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–∏" not in df.columns:
                    st.error("–í Excel –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å—Ç–æ–ª–±—Ü—ã '–ù–∞–∑–≤–∞–Ω–∏–µ', 'URL' –∏ '–§—Ä–∞–∑—ã –≤ —Ç–æ—á–Ω–æ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–∏'!")
                else:
                    df["–§—Ä–∞–∑—ã –≤ —Ç–æ—á–Ω–æ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–∏"] = ""
                    total = len(df)
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    for idx, row in df.iterrows():
                        name = str(row["–ù–∞–∑–≤–∞–Ω–∏–µ"]).strip()
                        phrases = get_serpstat_phrases_top_filtered(name, top_n=10)
                        df.at[idx, "–§—Ä–∞–∑—ã –≤ —Ç–æ—á–Ω–æ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–∏"] = "\n".join(phrases) if phrases else "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
                        progress = (idx + 1) / total
                        progress_bar.progress(progress)
                        status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx+1} –∏–∑ {total}...")
                    progress_bar.progress(1.0)
                    status_text.text("‚úÖ –ì–æ—Ç–æ–≤–æ! –ú–æ–∂–Ω–æ —Å–∫–∞—á–∏–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç.")
                    st.success("–ì–æ—Ç–æ–≤–æ! –°–∫–∞—á–∞–π Excel –Ω–∏–∂–µ üëá")
                    output = io.BytesIO()
                    df.to_excel(output, index=False)
                    st.download_button(
                        label="üíæ –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç",
                        data=output.getvalue(),
                        file_name="–∫–ª—é—á–µ–≤—ã–µ_—Ñ—Ä–∞–∑—ã_–ø–æ_—Ç–∞–±–ª–∏—Ü–µ.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

    # ========== –í–ö–õ–ê–î–ö–ê 2: –ê–Ω–∞–ª–∏–∑ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ ==========
    elif tab == "üßô –ê–Ω–∞–ª–∏–∑ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤":
        st.title("üßô –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ")
        st.info(
            "–≠—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –Ω–∞—Ö–æ–¥–∏—Ç —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤–æ –≤—Ç–æ—Ä–æ–º —Ç–µ–∫—Å—Ç–µ, –Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –ø–µ—Ä–≤–æ–º. "
            "–ê–Ω–∞–ª–∏–∑ —É—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤ (–ø–∞–¥–µ–∂–∏, —á–∏—Å–ª–∞), –ø—Ä–∏–≤–æ–¥—è –∏—Ö –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ (–ª–µ–º–º–µ)."
        )

        if 'text1' not in st.session_state:
            st.session_state.text1 = ""
        if 'text2' not in st.session_state:
            st.session_state.text2 = ""

        def clear_text1():
            st.session_state.text1 = ""

        def clear_text2():
            st.session_state.text2 = ""

        col1, col2 = st.columns(2)
        with col1:
            st.header("–¢–µ–∫—Å—Ç ‚Ññ1 (–û—Å–Ω–æ–≤–Ω–æ–π)")
            st.session_state.text1 = st.text_area(
                label="–°–ª–æ–≤–∞ –∏–∑ —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –±—É–¥—É—Ç –∏—Å–∫–ª—é—á–µ–Ω—ã –∏–∑ –∞–Ω–∞–ª–∏–∑–∞:",
                value=st.session_state.text1,
                height=300,
                placeholder="–í—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç...",
                key="text_area1"
            )
            st.button("–û—á–∏—Å—Ç–∏—Ç—å", on_click=clear_text1, key="clear_text1")

        with col2:
            st.header("–¢–µ–∫—Å—Ç ‚Ññ2 (–°—Ä–∞–≤–Ω–∏–≤–∞–µ–º—ã–π)")
            st.session_state.text2 = st.text_area(
                label="–ó–¥–µ—Å—å –±—É–¥—É—Ç –∏—Å–∫–∞—Ç—å—Å—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞:",
                value=st.session_state.text2,
                height=300,
                placeholder="–í—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...",
                key="text_area2"
            )
            st.button("–û—á–∏—Å—Ç–∏—Ç—å", on_click=clear_text2, key="clear_text2")

        if st.button("üöÄ –ù–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑", use_container_width=True):
            if st.session_state.text1 and st.session_state.text2:
                with st.spinner("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ, –∏–¥—ë—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤..."):
                    analysis_result = analyze_texts(st.session_state.text1, st.session_state.text2)
                st.header("–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞")
                st.markdown("---")
                st.markdown(analysis_result)
            else:
                st.error("‚ùó –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç—ã –≤ –æ–±–∞ –ø–æ–ª—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")

        st.markdown(
            """
            <style>
            .scroll-to-top {
                position: fixed;
                bottom: 20px;
                right: 20px;
                z-index: 1000;
            }
            </style>
            <button class="scroll-to-top" onclick="window.scrollTo({top: 0, behavior: 'smooth'})">‚Üë –ù–∞–≤–µ—Ä—Ö</button>
            """,
            unsafe_allow_html=True
        )

    # ========== –í–ö–õ–ê–î–ö–ê 3: –ü–æ–¥—Å–≤–µ—Ç–∫–∞ —Å–ª–æ–≤ + DOCX ==========
    elif tab == "üîç –ü–æ–¥—Å–≤–µ—Ç–∫–∞ —Å–ª–æ–≤ + DOCX":
        st.title("üîç –ü–æ–¥—Å–≤–µ—Ç–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ + –ü—Ä–æ—Å–º–æ—Ç—Ä + –≠–∫—Å–ø–æ—Ä—Ç –≤ Word")

        text_source = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ —Ç–µ–∫—Å—Ç–∞:", ["–í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é", "–ó–∞–≥—Ä—É–∑–∏—Ç—å .txt —Ñ–∞–π–ª"])

        if text_source == "–í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é":
            input_text = st.text_area("‚úçÔ∏è –í—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç", height=300)
        else:
            uploaded_file = st.file_uploader("üìÑ –ó–∞–≥—Ä—É–∑–∏—Ç–µ .txt —Ñ–∞–π–ª", type=["txt"])
            input_text = uploaded_file.read().decode("utf-8") if uploaded_file else ""

        keyword_block = st.text_area("üìã –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É):", height=200)
        keywords = [w.strip().lower() for w in keyword_block.strip().splitlines() if w.strip()]

        if st.button("‚ú® –ü–æ–¥—Å–≤–µ—Ç–∏—Ç—å –∏ –ø–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç"):
            if not input_text or not keywords:
                st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –∏ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤.")
            else:
                html_text = input_text
                for word in keywords:
                    pattern = re.compile(rf'\b({re.escape(word)}\w*)\b', flags=re.IGNORECASE)
                    html_text = pattern.sub(r'<span style="background-color:yellow;">\1</span>', html_text)
                st.markdown("### üëÄ –ü—Ä–æ—Å–º–æ—Ç—Ä —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π:")
                st.markdown(f"<div style='line-height:1.6'>{html_text}</div>", unsafe_allow_html=True)

                doc = Document()
                paragraph = doc.add_paragraph()
                words = re.split(r'(\W+)', input_text)
                for word in words:
                    clean_word = re.sub(r'\W+', '', word).lower()
                    match = any(re.fullmatch(rf'{re.escape(k)}\w*', clean_word, re.IGNORECASE) for k in keywords)
                    run = paragraph.add_run(word)
                    if match:
                        run.font.highlight_color = 7  # –∂—ë–ª—Ç—ã–π
                buffer = BytesIO()
                doc.save(buffer)
                buffer.seek(0)
                b64 = base64.b64encode(buffer.read()).decode()
                href = f'<a href="data:application/octet-stream;base64,{b64}" download="highlighted.docx">üì• –°–∫–∞—á–∞—Ç—å –∫–∞–∫ DOCX</a>'
                st.markdown("### üíæ –°–∫–∞—á–∞–π —Ñ–∞–π–ª, –µ—Å–ª–∏ –≤—Å—ë –æ–∫:")
                st.markdown(href, unsafe_allow_html=True)

    # ========== –í–ö–õ–ê–î–ö–ê 4: SEO Meta Checker ==========
    elif tab == "üîç SEO Meta Checker":
        st.title("SEO Meta Checker –¥–ª—è Apteka911")
        st.markdown("–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤ –∏ —Ñ—Ä–∞–∑ –Ω–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö —Å–∞–π—Ç–∞ Apteka911.")
        st.markdown("""
        <style>
        .info-box { background-color: #f8f9fa; border: 1px solid #e0e0e0; border-radius: 5px; padding: 10px; margin: 10px 0; }
        .result-content { margin: 5px 0; padding: 8px; background: white; border: 1px solid #eee; border-radius: 3px; font-size: 0.95em; word-wrap: break-word; }
        .result-content b { font-weight: 600; }
        .stTabs [data-baseweb="tab-list"] { gap: 8px; }
        .stTabs [data-baseweb="tab-list"] .stTabs [data-baseweb="tab-list"] { gap: 4px; }
        .stTabs [data-baseweb="tab"] { height: 50px; padding: 0 25px; margin-right: 0; border-radius: 5px 5px 0 0; }
        .stTabs [aria-selected="true"] { background-color: #e6f2ff; border-bottom: 2px solid #007bff; }
        .stMetric { border: 1px solid #ddd; border-radius: 5px; padding: 10px; background-color: #f9f9f9;}
        </style>
        """, unsafe_allow_html=True)

        if 'debug_messages' not in st.session_state: st.session_state.debug_messages = []
        if RUSSIAN_STEMMER is None and 'stemmer_warning_shown' not in st.session_state:
            st.warning("–†—É—Å—Å–∫–∏–π —Å—Ç–µ–º–º–µ—Ä (PyStemmer) –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω...")
            st.session_state.stemmer_warning_shown = True
        if not RAPIDFUZZ_AVAILABLE and 'rapidfuzz_warning_shown' not in st.session_state:
            st.warning("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ rapidfuzz –Ω–µ –Ω–∞–π–¥–µ–Ω–∞...")
            st.session_state.rapidfuzz_warning_shown = True

        if 'enable_lsi_truncation' not in st.session_state: st.session_state.enable_lsi_truncation = DEFAULT_ENABLE_LSI_TRUNCATION
        if 'lsi_trunc_max_remove' not in st.session_state: st.session_state.lsi_trunc_max_remove = DEFAULT_LSI_TRUNC_MAX_REMOVE
        if 'lsi_trunc_min_orig_len' not in st.session_state: st.session_state.lsi_trunc_min_orig_len = DEFAULT_LSI_TRUNC_MIN_ORIG_LEN
        if 'lsi_trunc_min_final_len' not in st.session_state: st.session_state.lsi_trunc_min_final_len = DEFAULT_LSI_TRUNC_MIN_FINAL_LEN
        if 'stem_fuzzy_ratio_threshold' not in st.session_state: st.session_state.stem_fuzzy_ratio_threshold = DEFAULT_STEM_FUZZY_RATIO_THRESHOLD

        debug_mode = st.sidebar.checkbox("üïµÔ∏è –í–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏", value=st.session_state.get('debug_mode_main_cb', False), key="debug_mode_main_cb")

        if debug_mode:
            st.sidebar.markdown("--- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ LSI ---")
            if RAPIDFUZZ_AVAILABLE:
                st.session_state.stem_fuzzy_ratio_threshold = st.sidebar.slider("–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –æ—Å–Ω–æ–≤ (%)",
                    min_value=50, max_value=100, value=st.session_state.stem_fuzzy_ratio_threshold, step=1, key="fuzzy_thresh_slider_ui")
            st.session_state.enable_lsi_truncation = st.sidebar.checkbox("–í–∫–ª—é—á–∏—Ç—å –ø–æ–∏—Å–∫ –ø–æ —É—Å–µ—á–µ–Ω–∏—é LSI",
                value=st.session_state.enable_lsi_truncation, key="enable_trunc_cb")
            if st.session_state.enable_lsi_truncation:
                st.session_state.lsi_trunc_max_remove = st.sidebar.slider("–ú–∞–∫—Å. —É–¥–∞–ª—è–µ–º—ã—Ö –±—É–∫–≤", 1, 5,
                    st.session_state.lsi_trunc_max_remove, key="trunc_max_rem_slider")
                st.session_state.lsi_trunc_min_orig_len = st.sidebar.slider("–ú–∏–Ω. –¥–ª–∏–Ω–∞ LSI —Å–ª–æ–≤–∞ –¥–ª—è —É—Å–µ—á–µ–Ω–∏—è", 5, 15,
                    st.session_state.lsi_trunc_min_orig_len, key="trunc_min_orig_slider")
                st.session_state.lsi_trunc_min_final_len = st.sidebar.slider("–ú–∏–Ω. –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞ –ø–æ—Å–ª–µ —É—Å–µ—á–µ–Ω–∏—è", 2, 5,
                    st.session_state.lsi_trunc_min_final_len, key="trunc_min_final_slider")

            st.sidebar.markdown("--- –û—Ç–ª–∞–¥–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ URL ---")
            st.session_state.manual_debug_url_val = st.sidebar.text_input("URL –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è HTML:",
                value=st.session_state.get("manual_debug_url_val", ""), key="manual_debug_url_input_field")
            st.session_state.manual_debug_lang_val = st.sidebar.selectbox("–Ø–∑—ã–∫ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ URL:", ["ru", "ua"],
                index=['ru','ua'].index(st.session_state.get("manual_debug_lang_val", 'ru')), key="manual_debug_lang_select")
            if st.sidebar.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å HTML –¥–ª—è URL", key="save_html_btn") and st.session_state.manual_debug_url_val:
                st.sidebar.info(f"HTML –¥–ª—è {st.session_state.manual_debug_lang_val.upper()}: {st.session_state.manual_debug_url_val}")
                get_page_data_for_lang(st.session_state.manual_debug_url_val, st.session_state.manual_debug_lang_val,
                                       debug_mode_internal=True,
                                       save_html_for_debug_manual=True, filename_prefix_manual="MANUAL_DEBUG_PAGE")
                st.sidebar.success("HTML (–µ—Å–ª–∏ –ø–æ–ª—É—á–µ–Ω) –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω.")

        uploaded_file = st.file_uploader("üì§ –ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏", type=["xlsx"])
        if 'processed_data' not in st.session_state: st.session_state.processed_data = None

        if uploaded_file and st.button("üöÄ –ù–∞—á–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –≤—Å–µ—Ö URL –∏–∑ —Ñ–∞–π–ª–∞", key="start_full_processing_btn"):
            st.session_state.debug_messages = []
            df_excel = None
            try:
                df_excel = pd.read_excel(uploaded_file)
                st.subheader(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞: '{uploaded_file.name}'")
                st.markdown(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –≤ —Ñ–∞–π–ª–µ: **{len(df_excel)}**. –ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:")
                st.dataframe(df_excel.head())
                with st.expander("–ü–æ–∏—Å–∫ URL –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–ª–∏—á–∏—è)", expanded=False):
                    url_to_search_in_df = st.text_input("–í–≤–µ–¥–∏—Ç–µ —á–∞—Å—Ç—å URL –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ —Ç–∞–±–ª–∏—Ü–µ:", key="df_url_search_input")
                    if url_to_search_in_df:
                        if COL_URL_RU_EXCEL not in df_excel.columns: st.error(f"–ö–æ–ª–æ–Ω–∫–∞ '{COL_URL_RU_EXCEL}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –≤–∞—à–µ–º Excel —Ñ–∞–π–ª–µ!")
                        else:
                            search_results_df = df_excel[df_excel[COL_URL_RU_EXCEL].astype(str).str.contains(url_to_search_in_df, case=False, na=False)]
                            if not search_results_df.empty: st.write(f"–ù–∞–π–¥–µ–Ω—ã —Å—Ç—Ä–æ–∫–∏ —Å '{url_to_search_in_df}':"); st.dataframe(search_results_df)
                            else: st.warning(f"URL, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π '{url_to_search_in_df}', –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ.")
                if COL_URL_RU_EXCEL not in df_excel.columns:
                    st.error(f"–í —Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–ê–Ø –∫–æ–ª–æ–Ω–∫–∞ '{COL_URL_RU_EXCEL}'!")
                    if debug_mode: display_debug_messages()
                    return

                load_progress_bar_ui = st.progress(0.0)
                load_progress_text_ui = st.empty()
                st.session_state.global_progress_text = "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏..."
                st.session_state.global_progress_value = 0.0
                load_progress_text_ui.info(st.session_state.global_progress_text)

                st.session_state.processed_data = load_all_pages_data_for_both_langs(df_excel, debug_mode)

                load_progress_text_ui.success(st.session_state.get('global_progress_text', "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞!"))
                load_progress_bar_ui.progress(st.session_state.get('global_progress_value', 1.0))

                urls_total_count = len(df_excel)
                urls_load_errors_count_ru = sum(1 for data_dict in st.session_state.processed_data.values() if data_dict.get('ru', {}).get('error'))
                urls_load_errors_count_ua = sum(1 for data_dict in st.session_state.processed_data.values() if data_dict.get('ua', {}).get('error'))
                st.subheader(f"üìä –û–±—â–∞—è —Å–≤–æ–¥–∫–∞ –ø–æ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü")
                summary_cols = st.columns(2)
                summary_cols[0].metric("–í—Å–µ–≥–æ URL –≤ —Ñ–∞–π–ª–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏", urls_total_count)
                summary_cols[1].metric("URL —Å –æ—à–∏–±–∫–∞–º–∏ –∑–∞–≥—Ä—É–∑–∫–∏ (RU)", urls_load_errors_count_ru, delta_color="inverse" if urls_load_errors_count_ru > 0 else "off")
                summary_cols[1].metric("URL —Å –æ—à–∏–±–∫–∞–º–∏ –∑–∞–≥—Ä—É–∑–∫–∏ (UA)", urls_load_errors_count_ua, delta_color="inverse" if urls_load_errors_count_ua > 0 else "off")

            except pd.errors.EmptyDataError: st.error("–û—à–∏–±–∫–∞: Excel —Ñ–∞–π–ª –ø—É—Å—Ç."); st.session_state.processed_data = None
            except KeyError as e: st.error(f"–û—à–∏–±–∫–∞: –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ '{str(e)}' –≤ Excel."); st.session_state.processed_data = None
            except Exception as e:
                st.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–ª–∏ –Ω–∞—á–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
                st.session_state.processed_data = None
                if 'debug_mode' in locals() and debug_mode:
                    st.exception(e)
                else:
                    print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ (debug_mode –Ω–µ –±—ã–ª –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –∏–ª–∏ False –≤ –º–æ–º–µ–Ω—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏—è): {e}")
                    traceback.print_exc()

        if uploaded_file and st.session_state.get('processed_data') is not None:
            df_for_tabs_display = None
            try:
                if 'df_excel' in locals() and df_excel is not None:
                    df_for_tabs_display = df_excel
                else:
                    df_for_tabs_display = pd.read_excel(uploaded_file)
            except Exception as e_read_tabs:
                st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–∫–ª–∞–¥–æ–∫: {e_read_tabs}")
                df_for_tabs_display = None

            if df_for_tabs_display is not None:
                main_ru_tab, main_ua_tab = st.tabs(["üá∑üá∫ –†—É—Å—Å–∫–∞—è –í–µ—Ä—Å–∏—è", "üá∫üá¶ –£–∫—Ä–∞–∏–Ω—Å–∫–∞—è –í–µ—Ä—Å–∏—è"])
                with main_ru_tab:
                    run_checks_for_language('ru', df_for_tabs_display, st.session_state.processed_data, debug_mode)
                with main_ua_tab:
                    run_checks_for_language('ua', df_for_tabs_display, st.session_state.processed_data, debug_mode)

        if 'debug_mode' in locals() and debug_mode:
            display_debug_messages()

if __name__ == "__main__":
    main()